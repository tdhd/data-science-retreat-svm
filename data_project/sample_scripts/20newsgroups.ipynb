{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test():\n",
    "    train = sklearn.datasets.fetch_20newsgroups(subset='train', remove=('headers'))\n",
    "    test = sklearn.datasets.fetch_20newsgroups(subset='test', remove=('headers'))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 7532)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.data), len(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n',\n",
       " 'rec.autos')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data[0], train.target_names[train.target[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 125145)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "data_encoder = text.TfidfVectorizer()\n",
    "X = data_encoder.fit_transform(train.data)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil/miniconda3/envs/svm/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier('hinge')\n",
    "clf.fit(X, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 125145)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_encoder.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.69      0.74       319\n",
      "           1       0.70      0.78      0.74       389\n",
      "           2       0.76      0.70      0.73       394\n",
      "           3       0.73      0.72      0.72       392\n",
      "           4       0.76      0.84      0.80       385\n",
      "           5       0.87      0.73      0.79       395\n",
      "           6       0.79      0.87      0.83       390\n",
      "           7       0.89      0.87      0.88       396\n",
      "           8       0.93      0.94      0.94       398\n",
      "           9       0.87      0.93      0.90       397\n",
      "          10       0.93      0.96      0.95       399\n",
      "          11       0.88      0.91      0.90       396\n",
      "          12       0.78      0.70      0.74       393\n",
      "          13       0.86      0.86      0.86       396\n",
      "          14       0.85      0.90      0.88       394\n",
      "          15       0.74      0.93      0.83       398\n",
      "          16       0.72      0.85      0.78       364\n",
      "          17       0.93      0.86      0.89       376\n",
      "          18       0.77      0.59      0.67       310\n",
      "          19       0.70      0.46      0.56       251\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      7532\n",
      "   macro avg       0.81      0.81      0.81      7532\n",
      "weighted avg       0.82      0.82      0.81      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test.target, clf.predict(data_encoder.transform(test.data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(__doc__)\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import learning_curve\n",
    "# from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "# def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "#                         n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "#     \"\"\"\n",
    "#     Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "#         An object of that type which is cloned for each validation.\n",
    "\n",
    "#     title : string\n",
    "#         Title for the chart.\n",
    "\n",
    "#     X : array-like, shape (n_samples, n_features)\n",
    "#         Training vector, where n_samples is the number of samples and\n",
    "#         n_features is the number of features.\n",
    "\n",
    "#     y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "#         Target relative to X for classification or regression;\n",
    "#         None for unsupervised learning.\n",
    "\n",
    "#     ylim : tuple, shape (ymin, ymax), optional\n",
    "#         Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "#     cv : int, cross-validation generator or an iterable, optional\n",
    "#         Determines the cross-validation splitting strategy.\n",
    "#         Possible inputs for cv are:\n",
    "#           - None, to use the default 3-fold cross-validation,\n",
    "#           - integer, to specify the number of folds.\n",
    "#           - An object to be used as a cross-validation generator.\n",
    "#           - An iterable yielding train/test splits.\n",
    "\n",
    "#         For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "#         :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "#         or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "#         Refer :ref:`User Guide <cross_validation>` for the various\n",
    "#         cross-validators that can be used here.\n",
    "\n",
    "#     n_jobs : int or None, optional (default=None)\n",
    "#         Number of jobs to run in parallel.\n",
    "#         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "#         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "#         for more details.\n",
    "\n",
    "#     train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "#         Relative or absolute numbers of training examples that will be used to\n",
    "#         generate the learning curve. If the dtype is float, it is regarded as a\n",
    "#         fraction of the maximum size of the training set (that is determined\n",
    "#         by the selected validation method), i.e. it has to be within (0, 1].\n",
    "#         Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "#         Note that for classification the number of samples usually have to\n",
    "#         be big enough to contain at least one sample from each class.\n",
    "#         (default: np.linspace(0.1, 1.0, 5))\n",
    "#     \"\"\"\n",
    "#     plt.figure()\n",
    "#     plt.title(title)\n",
    "#     if ylim is not None:\n",
    "#         plt.ylim(*ylim)\n",
    "#     plt.xlabel(\"Training examples\")\n",
    "#     plt.ylabel(\"Score\")\n",
    "#     train_sizes, train_scores, test_scores = learning_curve(\n",
    "#         estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     test_scores_mean = np.mean(test_scores, axis=1)\n",
    "#     test_scores_std = np.std(test_scores, axis=1)\n",
    "#     plt.grid()\n",
    "\n",
    "#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "#                      train_scores_mean + train_scores_std, alpha=0.1,\n",
    "#                      color=\"r\")\n",
    "#     plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "#                      test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "#     plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "#              label=\"Training score\")\n",
    "#     plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "#              label=\"Cross-validation score\")\n",
    "\n",
    "#     plt.legend(loc=\"best\")\n",
    "#     return plt\n",
    "\n",
    "\n",
    "# digits = load_digits()\n",
    "# X, y = digits.data, digits.target\n",
    "\n",
    "\n",
    "# title = \"Learning Curves (Naive Bayes)\"\n",
    "# # Cross validation with 100 iterations to get smoother mean test and train\n",
    "# # score curves, each time with 20% data randomly selected as a validation set.\n",
    "# cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "# estimator = GaussianNB()\n",
    "# plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "# title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# # SVC is more expensive so we do a lower number of CV iterations:\n",
    "# cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "# estimator = SVC(gamma=0.001)\n",
    "# plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20newsSGD_tfidf.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, '20newsSGD_SVM.joblib')\n",
    "joblib.dump(data_encoder, \"20newsSGD_tfidf.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
